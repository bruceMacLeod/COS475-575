{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bruceMacLeod/COS475-575/blob/main/Lab/Support%20Vector%20Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5bCRPilsy_T"
      },
      "source": [
        "#  Lab: Support Vector Machines\n",
        "\n",
        "In this lab, we'll use the ${\\tt SVC}$ module from the ${\\tt sklearn.svm}$ package to demonstrate the use of the support vector classifier and the SVM:\n",
        "\n",
        "Portions of this lab on Support Vector Machines are a Python adaptation of p. 359-366 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Original adaptation by J. Warmenhoven, updated by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016). \n",
        "\n",
        "\n",
        "### Lab team members : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N1dudfssy_X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# We'll define a function to draw a nice plot of an SVM\n",
        "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
        "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
        "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
        "\n",
        "    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
        "    # Support vectors indicated in plot by vertical lines\n",
        "    sv = svc.support_vectors_\n",
        "    plt.scatter(sv[:,0], sv[:,1], c='k', marker='x', s=100, linewidths='1')\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.show()\n",
        "    print('Number of support vectors: ', svc.support_.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj2vn4MVsy_Y"
      },
      "source": [
        "## Generate some data from random # generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtX8DPXjsy_Y"
      },
      "outputs": [],
      "source": [
        "# Generating random data: 20 observations of 2 features and divide into two classes.\n",
        "np.random.seed(5)\n",
        "X = np.random.randn(20,2)\n",
        "y = np.repeat([1,-1], 10)\n",
        "\n",
        "X[y == -1] = X[y == -1]+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr4wabi7sy_Z"
      },
      "source": [
        "Let's plot the data to see whether the classes are linearly separable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztynttwBsy_Z"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-6g5vqcsy_Z"
      },
      "source": [
        "Data is not completely seperable. \n",
        "\n",
        "### Exercise : \n",
        "    before we run SVC, where would you put the maximum margin line ? (discuss, nothing to hand in)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imp-ReA7sy_a"
      },
      "source": [
        "## Support Vector Classifier\n",
        "\n",
        "The ${\\tt SVC()}$ function can be used to fit a\n",
        "support vector classifier when the argument ${\\tt kernel=\"linear\"}$ is used.  The ${\\tt c}$ argument allows us to specify the cost of a violation to the margin\n",
        "\n",
        "* ${\\tt c}$ argument is **small**, then the margins will be wide and many support vectors will be on the margin or will violate the margin. \n",
        "* ${\\tt c}$ argument is large, then the margins willbe narrow and there will be few support vectors on the margin or violating the margin.\n",
        "\n",
        "We can use the ${\\tt SVC()}$ function to fit the support vector classifier for a\n",
        "given value of the ${\\tt cost}$ parameter. Here we demonstrate the use of this\n",
        "function on a two-dimensional example so that we can plot the resulting\n",
        "decision boundary. Let's start by generating a set of observations, which belong\n",
        "to two classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTkiWdHRsy_b"
      },
      "outputs": [],
      "source": [
        "svc = SVC(C=1, kernel='linear')\n",
        "svc.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2qVqK4wsy_b"
      },
      "source": [
        "We can now plot the support vector classifier by calling the ${\\tt plot\\_svc()}$ function on the output of the call to ${\\tt SVC()}$, as well as the data used in the call to ${\\tt SVC()}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMIIet7wsy_b"
      },
      "outputs": [],
      "source": [
        "plot_svc(svc, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUiUxVGjsy_c"
      },
      "source": [
        "The region of feature space that will be assigned to the âˆ’1 class is shown in\n",
        "light blue, and the region that will be assigned to the +1 class is shown in\n",
        "brown. The decision boundary between the two classes is linear (because we\n",
        "used the argument ${\\tt kernel=\"linear\"}$).\n",
        "\n",
        "The support vectors are plotted with crosses\n",
        "and the remaining observations are plotted as circles; we see here that there\n",
        "are 13 support vectors. We can determine their identities as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx3WKrmWsy_c"
      },
      "outputs": [],
      "source": [
        "svc.support_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N891335Nsy_c"
      },
      "source": [
        "What if we instead used a smaller value of the ${\\tt cost}$ parameter?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0eiYBdqsy_c"
      },
      "outputs": [],
      "source": [
        "svc2 = SVC(C=0.1, kernel='linear')\n",
        "svc2.fit(X, y)\n",
        "plot_svc(svc2, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwoJVaycsy_d"
      },
      "source": [
        "Now that a smaller value of the ${\\tt c}$ parameter is being used, we obtain a\n",
        "larger number of support vectors, because the margin is now **wider**.\n",
        "\n",
        "The ${\\tt sklearn.grid\\_search}$ module includes a a function ${\\tt GridSearchCV()}$ to perform cross-validation. In order to use this function, we pass in relevant information about the set of models that are under consideration. The\n",
        "following command indicates that we want perform 10-fold cross-validation to compare SVMs with a linear\n",
        "kernel, using a range of values of the cost parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W2zCaUXsy_d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Select the optimal C parameter by cross-validation\n",
        "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 5, 10, 100]}]\n",
        "clf = GridSearchCV(SVC(kernel='linear'), tuned_parameters, cv=10, scoring='accuracy')\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25USE8D6sy_d"
      },
      "source": [
        "We can easily access the cross-validation errors for each of these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLZ5ANVtsy_d"
      },
      "outputs": [],
      "source": [
        "clf.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofrr03Jssy_d"
      },
      "source": [
        "The ${\\tt GridSearchCV()}$ function stores the best parameters obtained, which can be accessed as\n",
        "follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cCxN6mqsy_e"
      },
      "outputs": [],
      "source": [
        "clf.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kq5jtYnsy_e"
      },
      "source": [
        "c=0.001 is best according to ${\\tt GridSearchCV}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e6oUqutsy_e"
      },
      "source": [
        "## Generate some test data and evaluate\n",
        "As usual, the ${\\tt predict()}$ function can be used to predict the class label on a set of\n",
        "test observations, at any given value of the cost parameter. Let's\n",
        "generate a test data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Sj4VIUsy_e"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "X_test = np.random.randn(20,2)\n",
        "y_test = np.random.choice([-1,1], 20)\n",
        "X_test[y_test == 1] = X_test[y_test == 1]-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UK-YHU0sy_e"
      },
      "source": [
        "Now we predict the class labels of these test observations. Here we use the\n",
        "best model obtained through cross-validation in order to make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgREcDk3sy_f"
      },
      "outputs": [],
      "source": [
        "svc2 = SVC(C=0.001, kernel='linear')\n",
        "svc2.fit(X, y)\n",
        "y_pred = svc2.predict(X_test)\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc2.classes_, columns=svc2.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVRDtSEJsy_f"
      },
      "source": [
        "With this value of ${\\tt c}$, 14 of the test observations are correctly\n",
        "classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLB3A8hzsy_f"
      },
      "source": [
        "\n",
        "## Non-Linear Kernels : Support Vector Machine\n",
        "\n",
        "In order to fit an SVM using a **non-linear kernel**, we once again use the ${\\tt SVC()}$\n",
        "function. However, now we use a different value of the parameter kernel.\n",
        "To fit an SVM with a polynomial kernel we use ${\\tt kernel=\"poly\"}$, and\n",
        "to fit an SVM with a radial kernel we use ${\\tt kernel=\"rbf\"}$. In the former\n",
        "case we also use the ${\\tt degree}$ argument to specify a degree for the polynomial\n",
        "kernel, and in the latter case we use ${\\tt gamma}$ to specify a\n",
        "value of $\\gamma$ for the radial basis kernel.\n",
        "\n",
        "Let's generate some data with a non-linear class boundary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbAtdwcXsy_f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(8)\n",
        "X = np.random.randn(200,2)\n",
        "X[:100] = X[:100] +2\n",
        "X[101:150] = X[101:150] -2\n",
        "y = np.concatenate([np.repeat(-1, 150), np.repeat(1,50)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=2)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVAsfr_Osy_f"
      },
      "source": [
        "See how one class is kind of stuck in the middle of another class? This suggests that we might want to use a **radial kernel** in our SVM. Now let's fit\n",
        "the training data using the ${\\tt SVC()}$ function with a radial kernel and $\\gamma = 1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRjpuBtosy_f"
      },
      "outputs": [],
      "source": [
        "svm = SVC(C=1.0, kernel='rbf', gamma=1)\n",
        "svm.fit(X_train, y_train)\n",
        "plot_svc(svm, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M7utQB4sy_f"
      },
      "source": [
        "The plot shows that the resulting SVM has a decidedly non-linear\n",
        "boundary. We can see from the figure that there are a fair number of training errors\n",
        "in this SVM fit. If we increase the value of cost, we can reduce the number\n",
        "of training errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld5_Zqdmsy_f"
      },
      "outputs": [],
      "source": [
        "# Increasing C parameter, allowing more flexibility\n",
        "svm2 = SVC(C=100, kernel='rbf', gamma=1.0)\n",
        "svm2.fit(X_train, y_train)\n",
        "plot_svc(svm2, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz_E5-0Usy_g"
      },
      "source": [
        "However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data. We can perform cross-validation using ${\\tt GridSearchCV()}$ to select the best choice of\n",
        "$\\gamma$ and cost for an SVM with a radial kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrfQNqETsy_g"
      },
      "outputs": [],
      "source": [
        "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],\n",
        "                     'gamma': [0.5, 1,2,3,4]}]\n",
        "clf = GridSearchCV(SVC(kernel='rbf'), tuned_parameters, cv=10, scoring='accuracy')\n",
        "clf.fit(X_train, y_train)\n",
        "clf.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8WrteTrsy_g"
      },
      "source": [
        "Therefore, the best choice of parameters involves ${\\tt cost=10}$ and ${\\tt gamma=0.5}$. We\n",
        "can plot the resulting fit using the ${\\tt plot\\_svc()}$ function, and view the test set predictions for this model by applying the ${\\tt predict()}$\n",
        "function to the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9XEdGphsy_g"
      },
      "outputs": [],
      "source": [
        "plot_svc(clf.best_estimator_, X_test, y_test)\n",
        "y_pred = clf.best_estimator_.predict(X_test)\n",
        "print(pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc2.classes_, columns=svc2.classes_))\n",
        "print(clf.best_estimator_.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "672ORSvLsy_g"
      },
      "source": [
        "85% of test observations are correctly classified by this SVM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQdOIpQ6sy_g"
      },
      "source": [
        "## ROC Curves\n",
        "\n",
        "The ${\\tt auc()}$ function from the ${\\tt sklearn.metrics}$ package can be used to produce ROC curves such as those we saw in lecture:\n",
        "Let's start by fitting two models, one more flexible than the other:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxAqfS2Psy_g"
      },
      "outputs": [],
      "source": [
        "# More constrained model\n",
        "svm3 = SVC(C=10, kernel='rbf', gamma=1)\n",
        "svm3.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC9DVKFZsy_h"
      },
      "outputs": [],
      "source": [
        "# More flexible model\n",
        "svm4 = SVC(C=10, kernel='rbf', gamma=50)\n",
        "svm4.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akwREBK-sy_h"
      },
      "source": [
        "SVMs and support vector classifiers output class labels for each observation.\n",
        "However, it is also possible to obtain fitted values for each observation,\n",
        "which are the numerical scores used to obtain the class labels. For instance,\n",
        "in the case of a support vector classifier, the fitted value for an observation\n",
        "$X = (X_1,X_2, . . .,X_p)^T$ takes the form $\\hat\\beta_0 + \\hat\\beta_1X_1 + \\hat\\beta_2X_2 + . . . + \\hat\\beta_pX_p$.\n",
        "\n",
        "In order to obtain the fitted values for a given SVM model fit, we\n",
        "use the ${\\tt .decision\\_function()}$ method of the SVC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCGTgxZasy_h"
      },
      "outputs": [],
      "source": [
        "y_train_score3 = svm3.decision_function(X_train)\n",
        "y_train_score4 = svm4.decision_function(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78GZ_VQXsy_h"
      },
      "source": [
        "Now we can produce the ROC plot to see how the models perform on both the training and the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K83DzOIcsy_h"
      },
      "outputs": [],
      "source": [
        "y_train_score3 = svm3.decision_function(X_train)\n",
        "y_train_score4 = svm4.decision_function(X_train)\n",
        "\n",
        "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3)\n",
        "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
        "\n",
        "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4)\n",
        "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
        "ax1.plot(false_pos_rate3, true_pos_rate3, label='SVM $\\gamma = 1$ ROC curve (area = %0.2f)' % roc_auc3, color='b')\n",
        "ax1.plot(false_pos_rate4, true_pos_rate4, label='SVM $\\gamma = 50$ ROC curve (area = %0.2f)' % roc_auc4, color='r')\n",
        "ax1.set_title('Training Data')\n",
        "\n",
        "y_test_score3 = svm3.decision_function(X_test)\n",
        "y_test_score4 = svm4.decision_function(X_test)\n",
        "\n",
        "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3)\n",
        "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
        "\n",
        "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4)\n",
        "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
        "\n",
        "ax2.plot(false_pos_rate3, true_pos_rate3, label='SVM $\\gamma = 1$ ROC curve (area = %0.2f)' % roc_auc3, color='b')\n",
        "ax2.plot(false_pos_rate4, true_pos_rate4, label='SVM $\\gamma = 50$ ROC curve (area = %0.2f)' % roc_auc4, color='r')\n",
        "ax2.set_title('Test Data')\n",
        "\n",
        "for ax in fig.axes:\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([-0.05, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETKC0sbIsy_h"
      },
      "source": [
        "## Application to \"Musk\" data\n",
        "\n",
        "The dataset that we will use for this example was provided by the UCI Machine Learning Repository and can be found here: <a href=\"https://archive.ics.uci.edu/ml/datasets/Musk+(Version+2)\">Musk (Version 2) Data Set</a> From Wikipedia (https://en.wikipedia.org/wiki/Musk) : Musk is a class of aromatic substances commonly used as base notes in perfumery. They include glandular secretions from animals such as the musk deer, numerous plants emitting similar fragrances, and artificial substances with similar odors. Musk was a name originally given to a substance with a strong odor obtained from a gland of the musk deer. \n",
        "\n",
        "This dataset describes a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be non-musks. The goal is to learn to predict whether new molecules will be musks or non-musks.  The original data is compressed, so we link to an uncompressed version.  Also, there are no missing values so we can skip that part of the data preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46HM9IW3sy_h"
      },
      "source": [
        "## Read in the data, set up train, test df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFUCAyDNsy_i"
      },
      "outputs": [],
      "source": [
        "musk_url = \"https://raw.githubusercontent.com/bruceMacLeod/COS475-575/main/Lab/data/musk_csv.csv\"\n",
        "df = pd.read_csv(musk_url) \n",
        "# Preview the first 5 lines of the loaded data \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag5q_N2gsy_i"
      },
      "outputs": [],
      "source": [
        "print(\"dimensions: \" , df.shape,\"\\n\")\n",
        "print(\"Breakdown by class:\")\n",
        "print(df.groupby('class').size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYiGKFSYsy_i"
      },
      "outputs": [],
      "source": [
        "X = df.iloc[:,3:169]\n",
        "y = df.iloc[:, 169]\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=.20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G13eCIRSsy_i"
      },
      "source": [
        "## Exercise : \n",
        "\n",
        "Okay, now it is your turn : \n",
        "\n",
        "* Does the data need to be scaled ? \n",
        "* Evaluate a linear Support Vector machine. Start by choosing an arbitrary **C** value and then use grid search to find the best value. Provide the confusion matrix for the best hyperparameters found from the grid search\n",
        "\n",
        "### Graduate Students/Extra Credit\n",
        "* Evaluate a non-linear Support Vector Machine. Start by choosing the kernel : for now poly or rbf. Provide a rationale for your choice. \n",
        "* Choose some arbitrary values for the hyperparameters and then use grid search to find the best value. Provide the confusion matrix for the best values found by the grid search.\n",
        "* Provide the ROC curves for a couple of reasonable options of the hyperparameters.  \n",
        "\n",
        "\n",
        "### Handin : \n",
        "\n",
        "One completed notebook for the breakout room group to Brightspace. Be sure to include the names of team members at the top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQe1nMh8sy_i"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Support Vector Machines.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}