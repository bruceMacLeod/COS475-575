{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bruceMacLeod/COS475-575/blob/main/Lab/CVDPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2-0TZbUH3XA"
      },
      "source": [
        "# <center> A Study of Factors related to Cardiovascular risk in Adults </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bqh9Q-_H3XE"
      },
      "source": [
        "The objective of this notebook is to show some of the essential steps of a workflow for building predictive models. The notebook provides a few examples of each step and it is only a very thin slice of what a complete analysis would consist of. \n",
        "\n",
        "The workflow includes:\n",
        "1. **Problem Definition**:  A clear definition of the problem enables us to identify the appropriate data to gather and technique(s) to use in order to solve the problem. For many problems this many require background reading, discussion with domain experts, and layered problem specification. \n",
        "2. **Data Gathering**: We have to know which data to use, where to gather them, and how to make them useful to solve our problem. In many cases, data from multiple sources can provide deeper insights. \n",
        "3. **Data Cleaning and Wrangling**: Raw data are generally incomplete, inconsistent, and contain many errors. Thus, we need to prepare the data for further processing. Data wrangling is the process of cleaning, structuring, and enriching raw data into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes, such as analytics.\n",
        "4. **Exploratory Data Analysis**: Exploratory data analysis (EDA) is an approach of performing initial investigations on our data. EDA normally has descriptive nature and uses graphical statistics to discover patterns, to identify anomalies, to test hypothesis, and to check assumptions regarding our data. \n",
        "\n",
        "5. **Data Modelling**:  Data modelling involves selecting and optiming the machine learning models that generate the best predictive performance based on the data we have. \n",
        "6. **Prediction**: Once we have developed the best predictive model, we can deploy it to make predictions.\n",
        "\n",
        "\n",
        "References : \n",
        "1. https://github.com/richasethi3/CVD_Prediction\n",
        "2. https://www.kaggle.com/ar2017/titanic-end-to-end-ml-workflow-top-7/notebook\n",
        "3. https://github.com/bruceMacLeod/COS475-575/blob/main/Assignment/HypertensionV1.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-lztOtCH3XF"
      },
      "source": [
        "# Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-lkEW5AH3XF"
      },
      "source": [
        "Cardiovascular disease (CVD) is a general term for conditions affecting the heart or blood vessels. It's usually associated with a build-up of fatty deposits inside the arteries (atherosclerosis) and an increased risk of blood clots. It can also be associated with damage to arteries in organs such as the brain, heart, kidneys and eyes.\n",
        "\n",
        "CVD is one of the main causes of death and disability in the USA, but it can often largely be prevented by leading a healthy lifestyle.\n",
        "\n",
        "The exact cause of CVD isn't clear, but there are lots of things that can increase your risk of getting it. These are called \"risk factors\". The more risk factors you have, the greater your chances of developing CVD. \n",
        "\n",
        "1.  **High blood pressure** (hypertension) is one of the most important risk factors for CVD. If your blood pressure is too high, it can damage your blood vessels.\n",
        "\n",
        "2.   **Smoking** Smoking and other tobacco use is also a significant risk factor for CVD. The harmful substances in tobacco can damage and narrow your blood vessels.\n",
        "\n",
        "3.   **High cholesterol** Cholesterol is a fatty substance found in the blood. If you have high cholesterol, it can cause your blood vessels to narrow and increase your risk of developing a blood clot.\n",
        "\n",
        "\n",
        "4.   **Diabetes** Diabetes is a lifelong condition that causes your blood sugar level to become too high. High blood sugar levels can damage the blood vessels, making them more likely to become narrowed.\n",
        "\n",
        "5.    **Inactivity** If you don't exercise regularly, it's more likely that you'll have high blood pressure, high cholesterol levels and be overweight. All of these are risk factors for CVD.\n",
        "\n",
        "6.    **Being overweight or obese** Being overweight or obese increases your risk of developing diabetes and high blood pressure, both of which are risk factors for CVD.\n",
        "\n",
        "We apply the tools of machine learning to predict the factors that are associated with cardiovascular disease in adults.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV_tz3bQH3XG"
      },
      "source": [
        "# Data Gathering and Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SuDlm0zEH3XH"
      },
      "outputs": [],
      "source": [
        "#Before moving to the next section, we need to import all packages required to do the analysis by calling the following:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "from sklearn.impute import SimpleImputer \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffLDliWH3XJ"
      },
      "source": [
        "### Gathering and Importing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRGZk4XGH3XJ"
      },
      "source": [
        "We need some functions to help automate the large number of files "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qi1prMwH3XK"
      },
      "outputs": [],
      "source": [
        "def download_data(data_dir, file_list):\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    for (year, data_file) in file_list:\n",
        "        sub_dir = os.path.join(data_dir, year)\n",
        "        if not os.path.exists(sub_dir):\n",
        "            os.makedirs(sub_dir)\n",
        "        url = 'http://wwwn.cdc.gov/Nchs/Nhanes/{0}/{1}.XPT'.format(year, data_file)\n",
        "        file_name = os.path.join(sub_dir, data_file + '.XPT')\n",
        "        if not os.path.exists(file_name):\n",
        "            logging.info('Downloading: {}'.format(url))\n",
        "            urllib.request.urlretrieve(url, file_name)\n",
        "        else:\n",
        "            logging.info('File exists: {}'.format(file_name))\n",
        "            \n",
        "def read_data_from_row(offset,ncols,col_list):\n",
        "    df = pd.DataFrame()\n",
        "    for i in range(ncols): \n",
        "        filename = LOCAL_DATA_PATH + file_list[offset + i][0] + \"/\" + file_list[offset + i][1] + \".XPT\"\n",
        "        one_year_df = pd.read_sas(filename)\n",
        "        df = pd.concat([df,one_year_df], axis=0)\n",
        "    df = df.loc[:, col_list]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcGJULN9H3XK"
      },
      "outputs": [],
      "source": [
        "file_list = [\n",
        "        ('2015-2016', 'DEMO_I'),    ('2017-2018', 'DEMO_J'),  ('2013-2014', 'DEMO_H'),\n",
        "        ('2015-2016', 'BPX_I'),     ('2017-2018', 'BPX_J'),   ('2013-2014', 'BPX_H'),\n",
        "        ('2015-2016', 'BMX_I'),     ('2017-2018', 'BMX_J') ,  ('2013-2014', 'BMX_H'),\n",
        "        ('2015-2016', 'TCHOL_I'),   ('2017-2018', 'TCHOL_J'), ('2013-2014', 'TCHOL_H'),\n",
        "        ('2015-2016', 'DIQ_I'),     ('2017-2018', 'DIQ_J'),   ('2013-2014', 'DIQ_H'),\n",
        "        ('2015-2016', 'SMQ_I'),     ('2017-2018', 'SMQ_J'),   ('2013-2014', 'SMQ_H'),\n",
        "        ('2015-2016', 'MCQ_I'),     ('2017-2018', 'MCQ_J'),   ('2013-2014', 'MCQ_H'),\n",
        "        ('2015-2016', 'HDL_I'),     ('2017-2018', 'HDL_J'),   ('2013-2014', 'HDL_H'),\n",
        "        ('2015-2016', 'TRIGLY_I'),  ('2017-2018', 'TRIGLY_J'),('2013-2014', 'TRIGLY_H'),\n",
        "        ('2015-2016', 'KIQ_U_I'),   ('2017-2018', 'KIQ_U_J'), ('2013-2014', 'KIQ_U_H')\n",
        "    ]\n",
        "\n",
        "demo_cols = ['SEQN', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH1', 'INDFMIN2']\n",
        "bpx_cols = ['SEQN', 'BPXPULS','BPXSY1', 'BPXDI1']\n",
        "bmx_cols = ['SEQN', 'BMXBMI', 'BMXWAIST']\n",
        "tchol_cols = ['SEQN', 'LBXTC']\n",
        "diab_cols = ['SEQN', 'DIQ010']\n",
        "smoking_cols = ['SEQN', 'SMQ020']\n",
        "heart_cols = ['SEQN', 'MCQ160B', 'MCQ160C', 'MCQ160D', 'MCQ160E', 'MCQ160F', 'MCQ300A']\n",
        "hdl_cols = ['SEQN', 'LBDHDD']\n",
        "trigly_cols = ['SEQN', 'LBXTR', 'LBDLDL']\n",
        "kidney_cols = ['SEQN', 'KIQ022']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRTqWng7H3XL"
      },
      "outputs": [],
      "source": [
        "LOCAL_DATA_PATH = os.path.join(\"datasets\", \"nhanes\") + \"/\"\n",
        "\n",
        "download_data(LOCAL_DATA_PATH, file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTNWFzLJH3XL"
      },
      "outputs": [],
      "source": [
        "demo_df = read_data_from_row(0,3,demo_cols)\n",
        "bpx_df = read_data_from_row(3,3,bpx_cols)\n",
        "bmx_df = read_data_from_row(6,3,bmx_cols)\n",
        "tchol_df = read_data_from_row(9,3,tchol_cols)\n",
        "diab_df = read_data_from_row(12,3,diab_cols)\n",
        "smoking_df = read_data_from_row(15,3,smoking_cols)\n",
        "heart_df = read_data_from_row(18,3,heart_cols)\n",
        "hdl_df = read_data_from_row(21,3,hdl_cols)\n",
        "trigly_df = read_data_from_row(24,3,trigly_cols)\n",
        "kidney_df = read_data_from_row(27,3,kidney_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxkjaeDZH3XL"
      },
      "source": [
        "### Merge the datatables into a single table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQcn2mvUH3XL"
      },
      "outputs": [],
      "source": [
        "pdList = [demo_df, bpx_df, bmx_df, hdl_df, trigly_df, tchol_df, diab_df,\n",
        "          kidney_df, heart_df, smoking_df]\n",
        "cvd_df = reduce(lambda x,y: pd.merge(x,y, on='SEQN', how='outer'), pdList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeuLqDM1H3XM"
      },
      "outputs": [],
      "source": [
        "#rename the columns to make the headers to something more meaningful\n",
        "cvd_df.rename(columns={'SEQN': 'seqn', 'RIAGENDR': 'gender', 'RIDAGEYR':'age',\n",
        "                   'RIDRETH1':'ethnicity', 'INDFMIN2':'income', 'BPXPULS':'pulse_regular',\n",
        "                   'BPXSY1':'sysbp', 'BPXDI1':'diabp', 'BMXBMI':'bmi',\n",
        "                   'BMXWAIST':'waistcircum', 'LBDHDD':'hdl', 'LBXTR':'trigly',\n",
        "                   'LBDLDL':'ldl', 'LBXTC':'totchol', 'DIQ010':'diabetes',\n",
        "                   'KIQ022':'kidney_fail', 'MCQ160B':'congestive_fail', 'MCQ160C':'coronary_disease',\n",
        "                   'MCQ160D':'angina', 'MCQ160E':'heart_attack',\n",
        "                   'MCQ160F':'stroke', 'MCQ300A':'fam_history', 'SMQ020':'smoking'}, inplace=True)\n",
        "cvd_df.set_index('seqn', inplace=True)\n",
        "cvd_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDitiWmXH3XM"
      },
      "source": [
        "###  Exploring Data Structure and Features\n",
        "Before performing data analysis, we often need to know the structure of our data. Therefore, we perform the following:\n",
        "- Viewing a small part of our datasets\n",
        "- Viewing data shape\n",
        "- Describing the features contained in the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFc9T-krH3XM"
      },
      "outputs": [],
      "source": [
        "cvd_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtb6mXNfH3XM"
      },
      "outputs": [],
      "source": [
        "# Only want to analysis on adults\n",
        "\n",
        "cvd_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjOf12LFH3XN"
      },
      "outputs": [],
      "source": [
        "#count and find the percentage of null values and concatenat the results\n",
        "missing = pd.concat([cvd_df.isnull().sum(), 100*cvd_df.isnull().mean()], axis=1)\n",
        "missing.columns = ['count', 'percentage']\n",
        "missing.sort_values(by='count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-67E3AH3XN"
      },
      "source": [
        "# Data Cleaning and Wrangling\n",
        "\n",
        "## Starting with the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffFtMHCXH3XN"
      },
      "outputs": [],
      "source": [
        "cvd_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_DSk8nz1H3XN"
      },
      "outputs": [],
      "source": [
        "#filering out the ldl null values from the dataset\n",
        "cvd_df = cvd_df[cvd_df['ldl'].notna()].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvO6klImH3XN"
      },
      "outputs": [],
      "source": [
        "cvd_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XiCisiFH3XO"
      },
      "outputs": [],
      "source": [
        "#look at the count and percentage of missing values again\n",
        "missing = pd.concat([cvd_df.isnull().sum(), 100*cvd_df.isnull().mean()], axis=1)\n",
        "missing.columns = ['count', 'percentage']\n",
        "missing.sort_values(by='count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4XtXk8nH3XO"
      },
      "source": [
        "Taking out the missing ldl values has taken care of missing trigly values and the rest of the missing columns as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXr-tvzHH3XO"
      },
      "outputs": [],
      "source": [
        "cvd_df = cvd_df[cvd_df['heart_attack'].notna()].reset_index(drop=True)\n",
        "#look at the count and percentage of missing values again\n",
        "missing = pd.concat([cvd_df.isnull().sum(), 100*cvd_df.isnull().mean()], axis=1)\n",
        "missing.columns = ['count', 'percentage']\n",
        "missing.sort_values(by='count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPGnRccAH3XO"
      },
      "source": [
        "This has resulted in reduction of our dataset by over 75%. Let's impute the rest of the missing sysbp, diabp, bmi and waistcircum columns by the median. Impute pulse_regular by mode, and forward fill the income column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49d9rCqbH3XO"
      },
      "outputs": [],
      "source": [
        "cvd_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUjHphR7H3XP"
      },
      "outputs": [],
      "source": [
        "cvd_df[['sysbp', 'diabp', 'bmi', 'waistcircum']] = cvd_df[['sysbp', 'diabp', 'bmi',\n",
        "                                                           'waistcircum']].fillna(cvd_df[['sysbp', 'diabp',\n",
        "                                                                                          'bmi', 'waistcircum']].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr2wV1WTH3XP"
      },
      "outputs": [],
      "source": [
        "cvd_df['pulse_regular'].fillna(cvd_df['pulse_regular'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvg3oLdrH3XP"
      },
      "outputs": [],
      "source": [
        "cvd_df['income'].fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svArPFiKH3XP"
      },
      "outputs": [],
      "source": [
        "cvd_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgYOwXrAH3XP"
      },
      "outputs": [],
      "source": [
        "dtype_cols = [col for col in cvd_df.columns if col not in ['bmi', 'waistcircum']]\n",
        "\n",
        "for col in dtype_cols:\n",
        "    cvd_df[col] = cvd_df[col].astype('int')\n",
        "cvd_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwl49DfrH3XP"
      },
      "outputs": [],
      "source": [
        "#let's get the histograms to get an idea of their distributions\n",
        "cvd_df.hist(figsize=(15, 10), color='rebeccapurple')\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQUvOcJQH3XP"
      },
      "source": [
        "### filter out rows, columns, recode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq48GePQH3XQ"
      },
      "outputs": [],
      "source": [
        "def hyper(sbp, dbp):\n",
        "    if ((sbp <= 130) and (dbp <= 80)):    \n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y1PYZMwH3XQ"
      },
      "outputs": [],
      "source": [
        "#def a function which takes different heart condions as input and returns 1 if either one is true\n",
        "#this is going to be our target\n",
        "def CVD(heart_1, heart_2, heart_3, heart_4, heart_5):\n",
        "    if ((heart_1 == 1) or (heart_2 == 1) or (heart_3 == 1)\n",
        "       or (heart_4 == 1) or (heart_5 == 1)):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv8V2d92H3XQ"
      },
      "outputs": [],
      "source": [
        "def cvd_recode_values(cvd_df):\n",
        "    #replace the 1(male) from the original dataset to 0 and 2(female) to 1\n",
        "    cvd_df['gender'].replace({1: 0, 2: 1}, inplace=True)\n",
        " \n",
        "    # replace 13(under $20,000) by 4($15,000 to $19,999)\n",
        "    #replace 12(over $20,000), 77(refused) and 99(don't know) by mode($25,000 to $34,999). \n",
        "    cvd_df['income'].replace({13: 4, 12: 6, 77: 6, 99: 6}, inplace=True)\n",
        " \n",
        "    #Here 1 means the individual has been told they have diabetes, 2 means no diabetes, 3 means borderline and 9 stands for refused\n",
        "    #replace 3 by 1 and 9 by 2\n",
        "    cvd_df['diabetes'].replace({3: 1, 9: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual has had kidney failure, 2 means no kidney failure and 9 stands for refused\n",
        "    #replace 9 by 2\n",
        "    cvd_df['kidney_fail'].replace({9: 2}, inplace=True)\n",
        "\n",
        "    # Here 1 means the individual has had congestive heart failure, 2 means no congestive heart failure and 9 stands for refused.\n",
        "    #replace 9 by 2\n",
        "    cvd_df['congestive_fail'].replace({9: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual has had coronary heart disease, 2 means no congestive coronary heart disease and 9 stands for refused.\n",
        "    cvd_df['coronary_disease'].replace({9: 2, 7: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual has had angina, 2 means no angina and 9 stands for refused.\n",
        "    cvd_df['angina'].replace({9: 2}, inplace=True)\n",
        "\n",
        "    # Here 1 means the individual has had heart attack, 2 means no heart attack and 9 stands for refused.\n",
        "    #replace 9 by 2\n",
        "    cvd_df['heart_attack'].replace({9: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual has had stroke, 2 means no stroke and 9 stands for refused.\n",
        "    #replace 9 by 2\n",
        "    cvd_df['stroke'].replace({9: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual has family history of heart disease, 2 means no family history of heart disease, 7 means don't know and 9 stands for refused.\n",
        "    #replace 9 and 7 by 2\n",
        "    cvd_df['fam_history'].replace({9: 2, 7: 2}, inplace=True)\n",
        "\n",
        "    #Here 1 means the individual smokes, 2 means no smoking, 7 means don't know and 9 stands for refused.\n",
        "    #replace 9 and 7 by 2\n",
        "    cvd_df['smoking'].replace({9: 2, 7: 2}, inplace=True)\n",
        "    return cvd_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOPro3uIH3XQ"
      },
      "outputs": [],
      "source": [
        "def cvd_add_attributes(cvd_df):\n",
        "    if ('congestive_fail' in cvd_df.columns) &  ('coronary_disease' in cvd_df.columns) & \\\n",
        "       ('angina' in cvd_df.columns) & ('heart_attack' in cvd_df.columns) & \\\n",
        "       ('stroke' in cvd_df.columns) :\n",
        "       cvd_df['CVD_risk'] = cvd_df.apply(lambda x: CVD(x['congestive_fail'], x['coronary_disease'],\n",
        "                                         x['angina'], x['heart_attack'], \n",
        "                                         x['stroke']), axis=1)\n",
        "    if ('sysbp' in cvd_df.columns) & ('diabp' in cvd_df.columns) : \n",
        "       cvd_df['hypertension_cat'] = cvd_df.apply(lambda x : hyper(x['sysbp'], x['diabp']), axis=1)\n",
        "    return cvd_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TZ3UUtPH3XQ"
      },
      "outputs": [],
      "source": [
        "def cvd_trim_rows(cvd_df):\n",
        "    # only want to do the analysis on adults\n",
        "    if ('age' in cvd_df.columns):\n",
        "        cvd_df = cvd_df[cvd_df.age >=20]\n",
        "    return cvd_df    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mIQvLYtH3XQ"
      },
      "outputs": [],
      "source": [
        "def cvd_drop_columns(cvd_df):\n",
        "        if ('congestive_fail' in cvd_df.columns) &  ('coronary_disease' in cvd_df.columns) & \\\n",
        "           ('angina' in cvd_df.columns) & ('heart_attack' in cvd_df.columns) & \\\n",
        "           ('stroke' in cvd_df.columns) :\n",
        "            cvd_df = cvd_df.drop(columns=['congestive_fail', 'coronary_disease', 'angina', 'heart_attack', 'stroke'], axis=1)# We do not need the seqn now (only needed for the merge)\n",
        "        if ('sysbp' in cvd_df.columns) & ('diabp' in cvd_df.columns) : \n",
        "            cvd_df = cvd_df.drop(columns=['sysbp','diabp'],axis=1)\n",
        "        return cvd_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBKtnDDgH3XR"
      },
      "outputs": [],
      "source": [
        "def cvd_add_trim__recode_drop(cvd_df):\n",
        "    cvd_df = cvd_trim_rows(cvd_df)\n",
        "    cvd_df = cvd_add_attributes(cvd_df)\n",
        "    cvd_df = cvd_recode_values(cvd_df)\n",
        "    cvd_df = cvd_drop_columns(cvd_df)\n",
        "    \n",
        "    return cvd_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5LiaOIvH3XR"
      },
      "outputs": [],
      "source": [
        "cvd_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfv8LbLGH3XR"
      },
      "outputs": [],
      "source": [
        "cvd_df = cvd_add_trim__recode_drop(cvd_df)\n",
        "cvd_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMg5DZsOH3XR"
      },
      "outputs": [],
      "source": [
        "#get the new histograms to get an idea of their distributions\n",
        "cvd_df.hist(figsize=(20, 15), color='rebeccapurple')\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.xticks(fontsize=12, color='black')\n",
        "plt.yticks(fontsize=12, color='black')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCaqEjqYH3XR"
      },
      "outputs": [],
      "source": [
        "#setting up a datapath for our file\n",
        "if not os.path.exists(LOCAL_DATA_PATH):\n",
        "    os.mkdir(datapath)\n",
        "datapath_cvd_data = os.path.join(LOCAL_DATA_PATH, 'cvd_data_cleaned.csv')\n",
        "if not os.path.exists(datapath_cvd_data):\n",
        "    cvd_df.to_csv(datapath_cvd_data, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4_p_E9iH3XR"
      },
      "source": [
        "### Now split into training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nDFQS9mH3XR"
      },
      "outputs": [],
      "source": [
        "y = cvd_df['CVD_risk']\n",
        "X = cvd_df.drop('CVD_risk',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_UlIXXaH3XR"
      },
      "outputs": [],
      "source": [
        "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train,  X_val, y_train, y_val = train_test_split(X_train,y_train,  test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdfqyM7KH3XR"
      },
      "outputs": [],
      "source": [
        "#confirming the ratios of train, test and validation sets for X\n",
        "print('Percent heldout for training:', round(100*(len(X_train)/len(cvd_df)),0),'%')\n",
        "print('Percent heldout for validation', round(100*(len(X_val)/len(cvd_df)),0),'%')\n",
        "print('Percent heldout for testing:', round(100*(len(X_test)/len(cvd_df)),0),'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmEN9r-kH3XS"
      },
      "outputs": [],
      "source": [
        "#confirming the ratios of train, test and validation sets for y\n",
        "print('Percent heldout for training:', round(100*(len(y_train)/len(cvd_df)),0),'%')\n",
        "print('Percent heldout for validation:', round(100*(len(y_val)/len(cvd_df)),0),'%')\n",
        "print('Percent heldout for testing:', round(100*(len(y_test)/len(cvd_df)),0),'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp0vTkejH3XS"
      },
      "source": [
        "# Exploratory Data Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhgJV35yH3XS"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw3sPuFwH3XS"
      },
      "outputs": [],
      "source": [
        "y_val.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ_BS1Z7H3XS"
      },
      "source": [
        "Our data is highly imbalanced with about 11% individuals with CVD risk. We may have to take this into account while modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3rilOdDH3XS"
      },
      "outputs": [],
      "source": [
        "\n",
        "Xy_train = X_train.copy()\n",
        "Xy_train['CVD_risk'] = y_train\n",
        "Xy_grouped = Xy_train.groupby('CVD_risk')[['age',  'bmi', 'waistcircum',\n",
        "                                   'hdl', 'trigly', 'ldl', 'totchol']].mean().reset_index()\n",
        "Xy_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzjo1M0mH3XS"
      },
      "outputs": [],
      "source": [
        "#def a function to plot barplots for the mean of grouped features\n",
        "def barplots(columns, ncol=None, figsize=(15, 8)):\n",
        "    if ncol is None:\n",
        "        ncol = len(columns)\n",
        "    nrow = int(np.ceil(len(columns) / ncol))\n",
        "    fig, axes = plt.subplots(nrow, ncol, figsize=figsize, squeeze=False)\n",
        "    fig.subplots_adjust(wspace=0.3, hspace=0.6)\n",
        "    for i, col in enumerate(columns):\n",
        "        ax = axes.flatten()[i]\n",
        "        barlist=ax.bar(x = 'CVD_risk', height = col, data=Xy_grouped)\n",
        "        barlist[0].set_color('forestgreen')\n",
        "        barlist[1].set_color('darkred')\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['No', 'Yes'], fontsize=14, color='black')\n",
        "        ax.set_xlabel('CVD risk',fontsize=14, color='black')\n",
        "        ax.set_ylabel(col, fontsize=14, color='black')\n",
        "    nsubplots = nrow * ncol    \n",
        "    for empty in range(i+1, nsubplots):\n",
        "        axes.flatten()[empty].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veeTkX_-H3XS"
      },
      "outputs": [],
      "source": [
        "features = [i for i in Xy_grouped.columns if i not in ['CVD_risk']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt-r0h8NH3XT"
      },
      "outputs": [],
      "source": [
        "barplots(features, ncol=4, figsize=(15, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkPF0cIdH3XT"
      },
      "outputs": [],
      "source": [
        "Xy_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XLhAkuxH3XT"
      },
      "outputs": [],
      "source": [
        "# Adding heatmaps\n",
        "corr = Xy_train[['gender', 'age',  'bmi', 'waistcircum', 'hdl', 'ldl', 'trigly', 'totchol',\n",
        "               'diabetes', 'kidney_fail', 'fam_history', 'smoking', 'CVD_risk']].corr()\n",
        "plt.figure(figsize=(24,10))\n",
        "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, cmap='BrBG', annot=True)\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':20}, pad=12)\n",
        "plt.xticks(fontsize=14, color='black')\n",
        "plt.yticks(fontsize=14, color='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcj_gv9H3XT"
      },
      "source": [
        "### More graph and EDA needed (todo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAVz22KH3XT"
      },
      "source": [
        "## Setup the pipeline to normalize values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyMOGXSbH3XT"
      },
      "source": [
        "### set missing values of numerical data to the median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zIaHvp-H3XT"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti7WGTs9H3XT"
      },
      "outputs": [],
      "source": [
        "X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q275x1meH3XT"
      },
      "outputs": [],
      "source": [
        "num_attribs = ['age', 'income', 'bmi', 'waistcircum', 'hdl', 'trigly', 'ldl', 'totchol']\n",
        "cat_attribs = [\"gender\",\"ethnicity\", \"pulse_regular\", 'diabetes', 'kidney_fail', 'fam_history', 'smoking', 'hypertension_cat']\n",
        "\n",
        "#cvd_num = cvd_df.drop(cat_attribs,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY_lof-oH3XT"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wFcGkCuH3XU"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "        ])\n",
        "cvd_prepared = full_pipeline.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr_n4d-3H3XU"
      },
      "outputs": [],
      "source": [
        "cvd_prepared.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVfb0CpGH3XU"
      },
      "outputs": [],
      "source": [
        "cvd_prepared[1,0:27]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM1X2NJ_H3XU"
      },
      "source": [
        "### Create a dataframe from the prepared numpy array so we can view data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fMAM0NcH3XU"
      },
      "outputs": [],
      "source": [
        "cvd_df[cat_attribs].apply(pd.Series.value_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgdLs-qAH3XU"
      },
      "outputs": [],
      "source": [
        "column_names = num_attribs.copy()\n",
        "column_names.extend(['Male','Female',\n",
        "                     'Mexican','Hispanic','White','Black','Other',\n",
        "                     'Pulse_regular', 'Pulse_irreg',\n",
        "                     'diabetes','no_diabetes',\n",
        "                     'kidney_fail', 'no_kidney_fail',\n",
        "                     'fam_history', 'no_fam_hist',\n",
        "                     'smoking', 'no_smoking',\n",
        "                     'hypertension', 'no_hypertension'])\n",
        "cvd_prepared_df = pd.DataFrame(cvd_prepared, columns=column_names)\n",
        "cvd_prepared_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rasmymsH3XU"
      },
      "source": [
        "#  Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Yd9-ftH3XU"
      },
      "source": [
        "### Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBWaK1jhH3XU"
      },
      "outputs": [],
      "source": [
        "\n",
        "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_clf.fit(cvd_prepared, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFOWnYcFH3XU"
      },
      "outputs": [],
      "source": [
        "some_data = cvd_prepared[5]\n",
        "some_labels = y_train.iloc[5]\n",
        "sgd_clf.predict([some_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKIyqj1wH3XV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(sgd_clf, cvd_prepared, y_train, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAcaS1MrH3XV"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg8vWzeTH3XV"
      },
      "source": [
        "### Exercise : Construct a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWid6uzUH3XV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjY5yT6_H3XV"
      },
      "source": [
        "### Exercise : Provide your thoughts on the numbers in the confusion matrix, including an analysis of why things may not be working out so well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK4gVwFrH3XV"
      },
      "source": [
        "## Precision and Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClOJXFA6H3XV"
      },
      "source": [
        "### Exercise : Calculate the Precision, Recall, and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGYoQNd7H3XV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBDzPHP8H3XV"
      },
      "source": [
        "### Provide your thoughts on precision, recall, and F1-score, including an analysis of why things may not be working out so well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqXNpC_NH3XV"
      },
      "source": [
        "## Precision/Recall Trade-off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAkK9qBgH3XV"
      },
      "source": [
        "### Exercise : Construct a graph of the Precision and Recall values as the threshold changes  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqBWnJPMH3XV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNmhU6cWH3XW"
      },
      "source": [
        "### Exercise : Why does the precision curve look jagged ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfRwfDOMH3XW"
      },
      "source": [
        "## The ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2ffe9ciH3XW"
      },
      "source": [
        "### Exercise : Construct and ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU-2JHltH3XW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXVPersnH3XW"
      },
      "source": [
        "### Graduate Students/Extra Credit : Choose another modeling technique, compare results on the ROC curve "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNNl30eFH3XW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CVDPrediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}